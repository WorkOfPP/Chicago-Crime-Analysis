# Chicago-Crime-Analysis

**<ins>Executive summary</ins>**

Ever since the 1960s, Chicago has experienced a rise in violent crime rate. By 2016, there are more homicides in Chicago than New York and Los Angeles combined [1](https://en.wikipedia.org/wiki/Crime_in_Chicago). Gun and drug issues are the main concerns of both the public and the government. According to public data, approximately 10 people are shot on an average day in Chicago.[2](https://www.chicagotribune.com/news/breaking/ct-homicide-victims-2017-htmlstory.html)

**<ins>Business UseCase</ins>**

The Chicago Police Department would like to address the increasing number of crimes reported in the city. They want to implement new regulations and deploy more police officers in vulnerable areas. This will be accomplished by analysis of reported crimes, including rates of crimes that resulted in arrests, types of crimes per district, and what time of day each block is more prone to crime. In addition, the Police Department would love to know which time of the year has the greatest number of violent crimes. This will be followed by an analysis of monthly data.

**<ins>Dataset</ins>**

Publicly available data source here: 
https://www.kaggle.com/datasets/chicago/chicago-crime and https://data.cityofchicago.org

**<ins>Database & Tools</ins>**

MySQL, SQL, Python, Tableau

<!---**<ins>Data Pipeline</ins>**-->


<!---**<ins>Exploratory Data Analysis</ins>**-->

**<ins>EER Diagram</ins>**

<!---**<ins>Dimensional Model</ins>**-->

**<ins>Tableau Dashboard</ins>**

[Crime Analysis Dashboard](https://public.tableau.com/app/profile/preetikap/viz/Final_Assignment_Chicago_Crime1/MainDashboard)
<!--- - Business case and objective(s)
The problem to be solved and datasets used
- Data Models
1.Conceptual, logical, and physical data models
2.Relational and Dimensional, (and if applicable No-SQL/Document, and Graph), and rationale behind using one over the other
3.The model should contain at least 5 tables (more is fine), which are completely Normalized till 3rd normal (if applicable) form and ER diagram provided in the document
4.Any denormalization required in the physical model, if yes, why (provide rationale for doing so)
- Data Profiling
Perform high level data profiling and cleaning, document any observations along with sample values such as outliers, data quality anomalies, any aggregations already applied in the data set or you plan on calculating
- Methodology and various tools used in the process
1.Evaluation and rationale behind using a certain Methodology, tool, and technology
2.Automation methodology (if any, or recommended) for the End to End pipeline.
- Insights

- Any recommendations and lessons learned>
